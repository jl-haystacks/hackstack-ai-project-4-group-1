{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: Home Depot, Lowes, Tool Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# County codes for AK, AL, AR, AZ, CA, CO, and CT properly formatted as strings with \"0\" prefixes to be read as 5 digits\n",
    "# See full explanation of method here: \n",
    "# https://stackoverflow.com/questions/20025882/add-a-string-prefix-to-each-value-in-a-string-column-using-pandas\n",
    "hdl_data = pd.read_csv('JC_clean_data/JC_clean_Home_Depot_Lowes_Data.csv', dtype={\"county\": str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdl_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Perform Exploratory Data Analysis on the stores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a. What are the total store counts of Home Depot and Lowes?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Home Depot store count\n",
    "hdl_data.HDcount.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowes store count\n",
    "hdl_data.Lcount.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b. Create one dummy variable for Home Depot and one dummy variable for Lowes that identifies if the store is located in a county**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdl_data['HDexists'] = [1 if x > 0 else 0 for x in hdl_data.HDcount]\n",
    "hdl_data['Lexists'] = [1 if x > 0 else 0 for x in hdl_data.Lcount]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c. Which store is present in more counties?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sum of counties with Home Depot present\n",
    "hdl_data.HDexists.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum of counties with Lowes present\n",
    "hdl_data.Lexists.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lowes stores are present in more counties than Home Depot stores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Use a United States map with FIPS locations to plot the store locations of both Lowes and Home Depot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install these packages and dependencies:\n",
    "# conda install -c plotly plotly-geo\n",
    "# conda install -c conda-forge pyshp\n",
    "# conda install -c conda-forge geopandas\n",
    "\n",
    "# This package is going to be deprecated and therefore should not be used\n",
    "# from plotly.figure_factory._county_choropleth import create_choropleth\n",
    "\n",
    "from plotly.offline import init_notebook_mode, plot, iplot\n",
    "import plotly.express as px\n",
    "import geopandas\n",
    "\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotly and geopandas necessary for producing these choropleths\n",
    "# For plotly express to print maps, jsons must be used instead of FIPS values for producting county shapes\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "with urlopen('https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json') as response:\n",
    "    counties = json.load(response)\n",
    "\n",
    "# These values do not need to be cast in lists anymore  \n",
    "# hd_values = hdl_data.HDcount.tolist()\n",
    "# l_values = hdl_data.Lcount.tolist()\n",
    "# fips = hdl_data.county.tolist()\n",
    "\n",
    "hd_fig = px.choropleth_mapbox(hdl_data, geojson=counties, locations='county', color='HDcount',\n",
    "                            color_continuous_scale=\"Viridis\",\n",
    "                            range_color=(0, 12),\n",
    "                            mapbox_style=\"carto-positron\",\n",
    "                            zoom=3, center = {\"lat\": 37.0902, \"lon\": -95.7129},\n",
    "                            opacity=0.5,\n",
    "                            labels={'HDcount':'Number of Stores'})\n",
    "\n",
    "l_fig = px.choropleth_mapbox(hdl_data, geojson=counties, locations='county', color='Lcount',\n",
    "                            color_continuous_scale=\"Viridis\",\n",
    "                            range_color=(0, 12),\n",
    "                            mapbox_style=\"carto-positron\",\n",
    "                            zoom=3, center = {\"lat\": 37.0902, \"lon\": -95.7129},\n",
    "                            opacity=0.5,\n",
    "                            labels={'Lcount':'Number of Stores'})\n",
    "\n",
    "\n",
    "# Obsolete figure factory map production methods never to be used again \n",
    "# hd_fig = create_choropleth(\n",
    "#             fips=fips, values=hd_values, show_state_data=True,\n",
    "#             county_outline={'color': 'rgb(255,255,255)', 'width': 0.5},\n",
    "#             title='United States Home Depot Store Locations by County',\n",
    "#             legend_title='Number of Stores')\n",
    "\n",
    "# l_fig = create_choropleth(\n",
    "#             fips=fips, values=l_values, show_state_data=True,\n",
    "#             county_outline={'color': 'rgb(255,255,255)', 'width': 0.5},\n",
    "#             title='United States Lowes Store Locations by County',\n",
    "#             legend_title='Number of Stores')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a. What observations can you make from the map?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iplot(hd_fig)\n",
    "\n",
    "# If iplot doesn't show a figure, uncomment and run the code below\n",
    "# plot(hd_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iplot(l_fig)\n",
    "\n",
    "# If iplot doesn't show a figure, uncomment and run the code below\n",
    "# plot(l_fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like Mid to Southern California, Southwestern Arizona, and Florida are promissing places to open home improvement stores. Texas and Washington also have some counties that should be considered. There seems to be at least one county in most states that have a high number of Home Depot or Lowes stores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Create a linear regression model to identify the correlations among the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Impute missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hdl_data[hdl_data.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rows 1654, 2922, and 2950 are missing most data, so we should drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdl_data.drop([1654,2922,2950], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like these missing values exist because there was no population data in 2000. We can replace these missing values with the mean of the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdl_data[hdl_data.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdl_data.pct_U18_2000.fillna(hdl_data.pct_U18_2000.mean(), inplace=True)\n",
    "hdl_data.pctwhite_2000.fillna(hdl_data.pctwhite_2000.mean(), inplace=True)\n",
    "hdl_data.pctblack_2000.fillna(hdl_data.pctblack_2000.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into HD and L, keep only demographic data columns and target, our target for linear regression\n",
    "# is the numeric \"count\" feature\n",
    "hd_data = hdl_data.copy()\n",
    "l_data = hdl_data.copy()\n",
    "\n",
    "hd_data.drop(['areaname', 'county', 'state', 'r1', 'r2', 'Lcount', 'HDexists', 'Lexists'], axis=1, inplace=True)\n",
    "l_data.drop(['areaname', 'county', 'state', 'r1', 'r2', 'HDcount', 'HDexists', 'Lexists'], axis=1, inplace=True)\n",
    "\n",
    "hd_features = hd_data.drop(['HDcount'], axis=1)\n",
    "hd_target = hd_data.HDcount\n",
    "\n",
    "l_features = l_data.drop(['Lcount'], axis=1)\n",
    "l_target = l_data.Lcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "# Scale the data using Pipeline with a StandardScaler in a preprocessing stage\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Create the pipelines for both models set with LinearRegression() for both\n",
    "hd_lm = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
    "l_lm = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
    "\n",
    "# Fit the models correspondingly\n",
    "hd_lm.fit(hd_features, hd_target)\n",
    "l_lm.fit(l_features, l_target)\n",
    "\n",
    "# Obsolete method soon to be deprecated\n",
    "# hd_lm = LinearRegression(normalize=True).fit(hd_features, hd_target)\n",
    "# l_lm = LinearRegression(normalize=True).fit(l_features, l_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "print(cross_val_score(hd_lm, hd_features, hd_target, cv=5))\n",
    "print(cross_val_score(l_lm, l_features, l_target, cv=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a. What customer demographic variables are most import to Lowes?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Access coefficients in make_pipeline model by calling steps[1][1] as explained here:\n",
    "# https://stackoverflow.com/questions/34373606/scikit-learn-coefficients-polynomialfeatures\n",
    "l_coeficients = zip(list(l_features), l_lm.steps[1][1].coef_)\n",
    "sorted(list(l_coeficients), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lowes is very interested in building stores in locations where there is a high percentage of people under the age of 18. Maybe this is an indicator that new families live in these areas and will be investing a lot of time and money into their homes over a longer period of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b. What customer demographic variables are most import to Home Depot?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Access coefficients in make_pipeline model by calling steps[1][1] as explained here:\n",
    "# https://stackoverflow.com/questions/34373606/scikit-learn-coefficients-polynomialfeatures\n",
    "hd_coeficients = zip(list(hd_features), hd_lm.steps[1][1].coef_)\n",
    "sorted(list(hd_coeficients), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Home Depot is interested in the percentage of different races of people living in the area. This chain also values percentage of people in college."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c. How are the chains similar in their decision making?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both chains seem to use percentage of people of different race in their decision making. They also seem to be interested in areas where there is a higher percentage of young people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d. How are they different?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Home Depot values home owners percentage in 2000 higher than Lowes. In addition, Lowes seems to value 2000 density more than Home Depot. It's interesting that both chains values the 2000 demographic statistics over the 2010 data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4:\tWhat are the top 5 towns / cities that can be predicted as potential candidates for new locations for both Lowes and Home Depot? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into HD and L, keep only demographic data columns and target, our target for logistic regression\n",
    "# is the boolean \"exists\" feature\n",
    "hd_data = hdl_data.copy()\n",
    "l_data = hdl_data.copy()\n",
    "\n",
    "hd_data.drop(['areaname', 'county', 'state', 'r1', 'r2', 'Lcount', 'HDcount', 'Lexists'], axis=1, inplace=True)\n",
    "l_data.drop(['areaname', 'county', 'state', 'r1', 'r2', 'Lcount', 'HDcount', 'HDexists'], axis=1, inplace=True)\n",
    "\n",
    "hd_X = hd_data.drop(['HDexists'], axis=1)\n",
    "hd_y = hd_data.HDexists\n",
    "\n",
    "l_X = l_data.drop(['Lexists'], axis=1)\n",
    "l_y = l_data.Lexists\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into train and test portions to test accuracy\n",
    "hd_X_train, hd_X_test, hd_y_train, hd_y_test = train_test_split(hd_X, hd_y, random_state=42)\n",
    "l_X_train, l_X_test, l_y_train, l_y_test = train_test_split(l_X, l_y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "hd_logit = LogisticRegressionCV(Cs=10, cv=5, class_weight='balanced', max_iter=1000, random_state=42).fit(hd_X_train, hd_y_train)\n",
    "l_logit = LogisticRegressionCV(Cs=10, cv=5, class_weight='balanced', max_iter=1000, random_state=42).fit(l_X_train, l_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Home Depot store predicition accuracy\n",
    "hd_logit.score(hd_X_test, hd_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowes store predicition accuracy\n",
    "l_logit.score(l_X_test, l_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy for predicting the test portions of the data looks pretty solid so we can train on the whole data to predict the next best store locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hd_full_logit = LogisticRegressionCV(Cs=5, cv=5, class_weight='balanced', max_iter=1000, random_state=42).fit(hd_X, hd_y)\n",
    "l_full_logit = LogisticRegressionCV(Cs=5, cv=5, class_weight='balanced', max_iter=1000, random_state=42).fit(l_X, l_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the target for all locations and extract the probability that a store should be built\n",
    "hd_store_prob = [x[1] for x in hd_full_logit.predict_proba(hd_X)]\n",
    "l_store_prob = [x[1] for x in l_full_logit.predict_proba(l_X)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the probability features to the original dataset so we can find the best next locations to build stores\n",
    "hdl_data['hd_store_prob'] = hd_store_prob\n",
    "hdl_data['l_store_prob'] = l_store_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For Home Depot, show only locations where there are no Home Depot stores,\n",
    "# sort by hd_store_prob descending and show top 5 areas predicted to be the best store locations\n",
    "hdl_data[hdl_data.HDexists == 0].sort_values(by='hd_store_prob', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression model predicts that San Francisco, LA; Union, NC; Arlington, VA; Elkhart, IN; and Sangamon, IL are the top 5 locations where Home Depot is **not** already located to build new stores. If Home Depot is concerned about competition from Lowes stores in the area, we could also inspect locations where neither Home Depot nor Lowes stores exist. But these top areas have few stores, with the exception of Union, NC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For Lowes, show only locations where there are no Lowes stores,\n",
    "# sort by l_store_prob descending and show top 5 areas predicted to be the best store locations\n",
    "hdl_data[hdl_data.Lexists == 0].sort_values(by='l_store_prob', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression model predicts that Westchester, NY; Dane, WI; Denver, CO; Essex, NJ; and Santa Barbara, CA are the top 5 locations where Lowes is **not** already located to build new stores. These locations seem, especially Westchester, NY seem to be saturated with Home Depot stores, so it may be wiser to choose locations with lower probabilities but less competition from the other chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5. Where should “Tool Time” build its next 5 stores based on the Census Data on your customers? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column that is the sum of the two probability columns for sorting purposes\n",
    "hdl_data['prob_sum'] = hdl_data.hd_store_prob + hdl_data.l_store_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# For Tool Time, show only locations where there are 1 or no stores for HD and Lowes,\n",
    "# sort by both l_store_prob and hd_store_prob descending and show top 5 areas predicted to be the best store locations\n",
    "hdl_data[(hdl_data.Lcount <= 1) & (hdl_data.HDcount <= 1)].sort_values(by='prob_sum', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression model predicts that Pinal, AZ; Ramsey, MN; Weld, CO; Webb, TX; and Ingham, MI are the top 5 locations to build new Tool Time stores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a. Explain your rational for your decision**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to find locations where both Home Depot and Lowes would consider building new stores, but we don't want to build in an area that is already saturated with other chains' stores. For this reason, I filtered the data by locations that had 1 or no stores for both chains. Then I sorted by probability that the location is a good place to build a store. This gives us locations that are promising for building stores, but have low stauration in terms of competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6. realtor.com market hotness index report "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a. Using the realtor.com market hotness index report from August of 2018 create an additional variable to segment the country into the following regions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# County codes for AK, AL, AR, AZ, CA, CO, and CT properly formatted as strings with \"0\" prefixes to be read as 5 digits\n",
    "# See full explanation of method here: \n",
    "# https://stackoverflow.com/questions/20025882/add-a-string-prefix-to-each-value-in-a-string-column-using-pandas\n",
    "# For dropping unwanted \"Unnamed 0\" index column, see here:\n",
    "# https://stackoverflow.com/questions/36519086/how-to-get-rid-of-unnamed-0-column-in-a-pandas-dataframe-read-in-from-csv-fil\n",
    "\n",
    "# Load the data\n",
    "state_region = pd.read_csv('JC_clean_data/state_region.csv', dtype={\"State Code\": str})\n",
    "realtor_data = pd.read_csv('JC_clean_data/JC_clean_RDC_MarketHotness_Monthly.csv', dtype={\"CountyFIPS\": str, \"State Code\": str})\n",
    "\n",
    "# Keep only relevant columns from state_region\n",
    "state_region.drop(['State', 'Division'], axis=1, inplace=True)\n",
    "\n",
    "import re\n",
    "\n",
    "# Drop the United States row\n",
    "realtor_data = realtor_data[realtor_data.CountyName != 'United States']\n",
    "# Use regex to extract the state abbreviation from \"ZipName\" in realtor_data and make a new column\n",
    "realtor_data['State Code'] = [re.search('\\w+, ([A-Z]{2})', x).group(1) for x in realtor_data.ZipName]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Properly merge the two dataframes by filtering out duplicates, since the copy=False parameter was not working\n",
    "# Explained here: https://stackoverflow.com/questions/19125091/pandas-merge-how-to-avoid-duplicating-columns\n",
    "\n",
    "# Join the state_region and realtor_data dataframes on 'State Code'\n",
    "realtor_data = realtor_data.merge(state_region, how='left', on='State Code', \n",
    "                                  suffixes=('', '_DROP')).filter(regex='^(?!.*_DROP)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the new last column\n",
    "realtor_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate FIPS codes, keep first\n",
    "realtor_data.drop_duplicates(subset='CountyFIPS', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index and show new data\n",
    "realtor_data.reset_index(drop=True, inplace=True)\n",
    "realtor_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b. Exploratory Data Analysis for realator.com data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**i. Which Region of the country has the best “Demand Score”**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by \"Region\", find mean of \"Demand Score\"\n",
    "realtor_data.groupby('Region')['Demand Score'].agg(np.mean).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Northeast has the best mean \"Demand Score.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ii. Which State in the country has the best “Demand Score”**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Group by \"State Code\", find mean of \"Demand Score\"\n",
    "realtor_data.groupby('State Code')['Demand Score'].agg(np.mean).sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Massachusetts has the best mean \"Demand Score.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**iii. Which metro area (pop_2010 > 1million) has the best “Demand Score”**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using the FIPS codes, join the POP_2010 column from hdl_data to realtor_data\n",
    "pop_data = hdl_data[['county', 'pop_2010']]\n",
    "realtor_withpop = pd.merge(realtor_data, pop_data, how='inner', left_on='CountyFIPS', right_on='county')\n",
    "\n",
    "# Filter by pop_2010 > 1mil, then find highest \"Demand Score\"\n",
    "realtor_withpop[realtor_withpop.pop_2010 > 1e6][['CountyName', 'State Code','Demand Score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Middlesex, MA has the best \"Demand Score\" of metro areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c.\tCompare and contrast these findings with your predicted new store findings.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**i. Describe your findings as they relate to the customer attributes and potential business opportunity that Lowes, Home Depot and/or Tool Time may have if they are or are not located in the areas that have high demands for real estate opportunities**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the Northeast, Texas, Ohio, California, and Florida are all good areas for stores to be loacted, according to \"Demand Score.\" This somewhat agrees with the model predictions; there were a lot of predicted locations in California and the Northeast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d. Add the following as features to the original HDLo data set and predict again where Tool Time should build its next 5 stores.**\n",
    "1.\tMedian.Listing.Price\n",
    "2.\tDemand.Score\n",
    "3.\tHotness.Score\n",
    "4.\tNieleson.HH.Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the realtor_data to only contain the columns we want to merge with the hdl_data\n",
    "for_merge = realtor_data[['CountyFIPS', 'Median Listing Price', 'Demand Score', 'Hotness Score', 'Nielsen HH Rank']]\n",
    "\n",
    "# Rename columns\n",
    "for_merge.columns = ['county', 'Median.Listing.Price', 'Demand.Score', 'Hotness.Score', 'Nieleson.HH.Rank']\n",
    "\n",
    "# Merge this data with hdl_data on county\n",
    "added_data = pd.merge(hdl_data, for_merge, how='inner', on='county', copy=False)\n",
    "# Drop previous store probability columns\n",
    "added_data.drop(columns=['hd_store_prob', 'l_store_prob', 'prob_sum'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdl_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy data into training and testing sets, drop earlier added probability columns along with other irrelevant columns\n",
    "hd_data = added_data.copy()\n",
    "l_data = added_data.copy()\n",
    "\n",
    "hd_data.drop(['areaname', 'county', 'state', 'r1', 'r2', 'Lcount', 'HDcount', 'Lexists'], axis=1, inplace=True)\n",
    "l_data.drop(['areaname', 'county', 'state', 'r1', 'r2', 'Lcount', 'HDcount', 'HDexists'], axis=1, inplace=True)\n",
    "\n",
    "hd_X = hd_data.drop(['HDexists'], axis=1)\n",
    "hd_y = hd_data.HDexists\n",
    "\n",
    "l_X = l_data.drop(['Lexists'], axis=1)\n",
    "l_y = l_data.Lexists\n",
    "\n",
    "# Split the data into train and test portions to test accuracy\n",
    "hd_X_train, hd_X_test, hd_y_train, hd_y_test = train_test_split(hd_X, hd_y, random_state=42)\n",
    "l_X_train, l_X_test, l_y_train, l_y_test = train_test_split(l_X, l_y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the models\n",
    "hd_logit = LogisticRegressionCV(Cs=10, cv=5, class_weight='balanced', max_iter=10000, random_state=42).fit(hd_X_train, hd_y_train)\n",
    "l_logit = LogisticRegressionCV(Cs=10, cv=5, class_weight='balanced', max_iter=10000, random_state=42).fit(l_X_train, l_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Home Depot store predicition accuracy\n",
    "hd_logit.score(hd_X_test, hd_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowes store predicition accuracy\n",
    "l_logit.score(l_X_test, l_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on the full data\n",
    "hd_full_logit = LogisticRegressionCV(Cs=10, cv=5, class_weight='balanced', max_iter=15000, random_state=42).fit(hd_X, hd_y)\n",
    "l_full_logit = LogisticRegressionCV(Cs=10, cv=5, class_weight='balanced', max_iter=15000, random_state=42).fit(l_X, l_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the target for all locations and extract the probability that a store should be built\n",
    "hd_store_prob = [x[1] for x in hd_full_logit.predict_proba(hd_X)]\n",
    "l_store_prob = [x[1] for x in l_full_logit.predict_proba(l_X)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the probability features to the original dataset so we can find the best next locations to build stores\n",
    "added_data['hd_store_prob'] = hd_store_prob\n",
    "added_data['l_store_prob'] = l_store_prob\n",
    "\n",
    "# Create new column that is the sum of the two probability columns for sorting purposes\n",
    "added_data['prob_sum'] = added_data.hd_store_prob + added_data.l_store_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For Tool Time, show only locations where there are 1 or no stores for HD and Lowes,\n",
    "# sort by prob_sum descending and show top 5 areas predicted to be the best store locations\n",
    "added_data[(added_data.Lcount <= 1) & (added_data.HDcount <= 1)].sort_values(by='prob_sum', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e. What are the top 5 new area names for which Tool Time should build their stores?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first four stores remain the same, however Ottawa, MI overtook Ingham, MI in this version of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f. Do these features increase the prediction accuracy for the new area predictions?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the new scores, the new features seem to have decreased the accuracy of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**g. Does overlaying the realtor data set add value to the business strategy of Tool Time?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, I would conclude that adding the new features **does not** add value to the business strategy that I would recommend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**h. Is there an alternative strategy that Tool Time should explore other than Census Data and Realtor data?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tool Time might want to consider areas that have high demand for commerical real estate, as new businesses will have contractors in need of supplies. Tool Time might also want to look into trends for up-and-coming neighborhoods where real estate prices may increase in the future, and get ahead of competitors by opening stores newly desired neighborhoods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
